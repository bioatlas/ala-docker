
# These IP addresses are never allowed to access locations except robots.txt
geo $blocked_ip {
    default 0;
    46.229.161.131 1;
}

map $http_user_agent $blocked_user_agent {
    default 0;
    # These user agents are never allowed access to this location
    # Telling NGINX to return 444 is a special pseudo response code that immediately cuts the TCP connection with no response
    "~Semrush" 1;
    "~SEMrush" 1;
    "~semrush" 1;
}

#Ansible managed
server {
    listen 80;
    listen [::]:80;
    server_name static.biodiversitydata.se;
    return 301 https://$server_name$request_uri;
}

server {
    listen 443 ssl http2;
    listen [::]:443 ssl http2;
    server_name static.biodiversitydata.se;
    access_log /var/log/nginx/access.log vhost2;
    ssl_certificate /etc/nginx/certs/biodiversitydata.se.crt;
    ssl_certificate_key /etc/nginx/certs/biodiversitydata.se.key;
    ssl_protocols  TLSv1.2  ;
    ssl_ciphers 'ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES256-SHA384:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA256';
    ssl_prefer_server_ciphers on;
    ssl_session_timeout 1d;
    ssl_session_cache shared:SSL:50m;
    ssl_session_tickets off;
    ssl_stapling on;
    ssl_stapling_verify on;

    # Force HTTPS was configured, so setting STS value to 1 year (31536000 seconds)
    add_header Strict-Transport-Security "max-age=31536000" always;

    # Secure referrer policy to avoid leaking paths, which can include auth ticket information
    add_header Referrer-Policy "strict-origin-when-cross-origin" always;

    # Framing security setting (Reenable using frame_options_header)
    add_header X-Frame-Options DENY always;

    # Disable browser sniffing away from the header declared content type (Reenable using content_type_options_header)
    add_header X-Content-Type-Options nosniff always;

    # If ssl is supported, then push this policy to all users who understand it to trigger HSTS
    add_header Content-Security-Policy upgrade-insecure-requests;

    location = /robots.txt {
        add_header  Content-Type  text/plain;
        return 200
"
# Disallow particular paths for all user agents
User-agent: *
Disallow: /
Crawl-delay: 30

# Disallow entire user agents
User-agent: Semrush
Disallow: /

User-agent: SEMrush
Disallow: /

User-agent: semrush
Disallow: /

";

}
    location / {
        if ($blocked_ip) {
            return 403;
        }
        if ($blocked_user_agent) {
            return 444;
        }
        include /etc/nginx/ala-cors.conf; 
        proxy_set_header Host $host;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_set_header X-Forwarded-Port $server_port;
        proxy_read_timeout 10m;
        proxy_pass http://sbdi-static:80;
    }
}
